import requests
from lxml import etree
import os
import re

url = "http://jkl.com.cn/cn/invest.aspx?id=888"
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68"
}
r = requests.get(url,headers=headers).text
html = etree.HTML(r)
# 项目名称
name = html.xpath('//div[@class="infoLis"]/ul/li/a/text()')
# 项目链接
urls = html.xpath('//div[@class="infoLis"]/ul/li/a/@href')
# print(name,urls)

# 进行处理
for u,v in zip(name,urls):
    name1 = u.strip().replace('/','.').replace('...','报表')
    PageUrl = "http://jkl.com.cn/cn/" + v
    # print(name1,PageUrl)
    path = './'+ name1
    if not os.path.exists(path):
        os.makedirs(path)
    # 爬取每个链接
    response = requests.get(PageUrl,headers=headers).text
    new_html = etree.HTML(response)
    # 获取尾页
    last = new_html.xpath('//a[text()="尾页"]/@href')
    if last != []:
        page = re.search("(\d+)'\)",last[0]).group(1)
    else:
        page = 1

    # 爬取每个文件的url
    for i in range(1,int(page)+1):
        data = {
            '__EVENTTARGET': 'AspNetPager1',
            '__EVENTARGUMENT': i
        }
        r1 = requests.get(url=PageUrl,headers=headers,params=data).text
        html1 = etree.HTML(r1)
        # 文件名称
        Filename = html1.xpath('//div[@class="newsLis"]//li/a/text()')
        FileUrls = html1.xpath('//div[@class="newsLis"]//li/a/@href')
        Filename = [Filename.strip() for Filename in Filename]
        # 处理空列表的形式
        if all(FileUrls):
            Fileurls = ["http://jkl.com.cn"+j for j in FileUrls]
            # print(Fileurls)
            dict2 = dict(zip(Filename,Fileurls))
            # print(dict2)
            for a,b in dict2.items():
                r2 = requests.get(url=b,headers=headers).content
                path = path + '/' + a + '.' + 'pdf'
                with open(path,'wb') as f:
                    f.write(r2)
                    print(a,"下载成功")
